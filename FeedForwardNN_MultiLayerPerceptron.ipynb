{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we describe the another kind of feedforward Neural Network, the Multi-layer Perceptron or MLP. The structure of the notebook is:\n",
    "\n",
    "- [Theoretical Analysis](#s1)\n",
    "    - [Algorithm](#s1.1)\n",
    "    - [Practical Tips](#s1.2)\n",
    "- [Implementation in Python](#s2)\n",
    "- [Mini-challenge](#s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a class=\"anchor\" id='s1'> Theoretical Analysis </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Supervised Machine Learning, __multilayer perceptron (MLP)__,  is one of the primary machine learning techniques for both regression and classification.\n",
    "\n",
    "In the context of Neural Networks, a multilayer perceptron is a class of feedforward Neural Network. The term MLP is used ambiguously, sometimes loosely to refer to any feedforward NN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation). We use here the name MLP to describe a feedforward NN with an arbitrary number of hidden layers and for arbitrary activation function without referring necessarly to the step function used in the single layer perceptron.\n",
    "\n",
    "Therefore, an MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training as we will see. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n",
    "\n",
    "The following defines a prototypical M-layer (M-2 hidden layers) MLP with a one-dimensional output and a $p$-dimensional input\n",
    "\n",
    "- The output perceptron has an activation function $g_o$ and hidden layer nodes have activation function $g$\n",
    "\n",
    "- Every perceptron in layer $l_{k-1}$; layers are \"fully connected.\" Thus, every perceptron depends on the outputs of all the perceptrons in the previous layer (this is without loss of generality since the weight connecting two perceptrons can still be zero, which is the same as no connection being present)\n",
    "\n",
    "- There are no connections between perceptrons in the same layer\n",
    "\n",
    "\n",
    "Schematically, a four-layer MLP can the drawn as\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img width=500 src='../NeuralNetwork/images/mlp.png' /> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a class=\"anchor\" id='s1.1'> Algorithm </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before entering in the details of the algorithm we give some definition\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "x_i&:\\quad\\text{$i$-th $(p+1)$-dimensional training input vector}\\,\\left(1,x_{1i},\\cdots, x_{pi}\\right)\\\\\n",
    "y_i&:\\quad\\text{$i$-th training output vector}\\\\\n",
    "w_{jm}^k&:\\quad\\text{weight for node $m$ in layer $l_k$ for incoming node $j$}\\\\\n",
    "b_{m}^k&:\\quad\\text{bias for node $m$ in layer $l_k$}\\\\\n",
    "w_{m}^k&:\\quad\\text{$j+1$-dimensional vector weight for node $m$ in layer $l_k$}\\\\\n",
    "o_m^k&:\\quad\\text{output for the node $m$ in the layer $l_k$}\\\\\n",
    "M_k&:\\quad\\text{number of nodes in layer $l_k$}\\\\\n",
    "g&:\\quad\\text{activation function for the hidden layer nodes}\\\\\n",
    "g_o&:\\quad\\text{activation function for the output layer nodes}\n",
    "\\end{align*}\n",
    "\n",
    "In the following, we leave the specification of the activation functions generic, even tough one of the common choice is the sigmoid function \n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "\\end{align*}\n",
    "\n",
    "Since we said that the MLP can be used both for regression or classification we leave also the specification of the loss function $L(y_i,\\hat{y}_i)$ generic.\n",
    "\n",
    "\n",
    "Then, computation and training of the MLP's output proced in two steps: the forward step where informations flows from input to output nodes producing an output result. The second step is call __backpropagation__ and is the training step, where the net has to adjust the weights in order to minimiaze the cost function error. \n",
    "\n",
    "##### FeedForward \n",
    "\n",
    "We describe it for a generic training point $(x,y)$. This algorithm has to be run for every training point $x_i$ in order to get $o_i$ outputs.\n",
    "\n",
    "0. Inizialize the layers $l_k$\n",
    "\n",
    "\n",
    "1. For $k$ from 1 to $K-1$:\n",
    "    \n",
    "    a. Compute $z_{m_k}^k=x\\cdot w^k_{m_k}$\n",
    "    \n",
    "    b. Compute $o_{m_k}^k=g(z_{m_k}^k)$\n",
    "   \n",
    "\n",
    "\n",
    "2. Compute the output $\\hat{y}_i$ for the output layer $l_K$\n",
    "\n",
    "    a. Compute $z_{1}^K=o_i\\cdot w^K_1$\n",
    "    \n",
    "    b. Compute $o=o_1^K=g_{o}(z_{1}^K)$\n",
    "\n",
    "where $m_k$ is the number of nodes in the $k$-th layer.\n",
    "\n",
    "Once we feed the net until we get the output we have to compute the risk function\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{R}(w)=\\sum_{i=1}^NL(y_i, o_i)\n",
    "\\end{align*}\n",
    "\n",
    "The backpropagation works as follows\n",
    "\n",
    "\n",
    "##### Backpropagation\n",
    "\n",
    "The derivation of the backpropagation algorithm begins by applying the chain rule to the error function partial derivative\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\mathcal{R}}{\\partial w^k_{m_k}} = \\frac{\\partial\\mathcal{R}}{\\partial z_{m_k}^k}\\frac{\\partial z_{m_k}^k}{\\partial w^k_{m_k}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "where again $z_m^k$ is scalar product happening at the $m$-th node of the $k$-th layer, before it is passed to the activation function. This decomposition of the partial derivative basically says that the change in the error function due to a weight is a product of the change in the risk function due to the activation $z_m^k$ times the change in the activation $z_m^k$ due to the weight $w^k_m$.  The first term is called error and ii's denoted by\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{m_k}^k=\\frac{\\partial\\mathcal{R}}{\\partial z_{m_k}^k}\n",
    "\\end{align*}\n",
    "\n",
    "The second term can be calculated from the equation for $z_m^k$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z_{m_k}^k}{\\partial w^k_{m_k}}=\\frac{\\partial}{\\partial w^k_{m_k}}\\left(o_{m_{k-1}}^{k-1}\\cdot w^k_{m_k}\\right)=o^{k-1}_{m_{k-1}}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, the partial derivative of the error function with respect to the weight is\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\mathcal{R}}{\\partial w^k_{m_k}} = \\delta_{m_{k}}^ko^{k-1}_{m_{k-1}}\n",
    "\\end{align*}\n",
    "\n",
    "The calculation of the error $\\delta_{m_{k}}^k$ will be shown to be dependent on the values of error terms in the next layer. Thus, computation of the error terms will proceed backwards from the output layer down to the input layer. This is where backpropagation, or backwards propagation of errors, gets its name.\n",
    "\n",
    "For the output layer, we have indeed\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{1}^K=\\frac{\\partial\\mathcal{R}}{\\partial z_{m_{K-1}}^{K-1}}=\\frac{\\partial\\mathcal{R}(o)}{\\partial o}g_{o}'(z_1^K)\n",
    "\\end{align*}\n",
    "\n",
    "For the hidden layers we use the chain rule for derivatives and we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{m_k}^k=g'(z_{m_k}^k)\\left[w^{k+1}_{m_{k+1}}\\cdot\\delta^{k+1}_{m_{k+1}}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "As we can see the error depends on the error at the next step. The weight at the $n$-th step of updating reads\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "w^{k}_{m_{k}(n+1)}=w^{k}_{m_{k}(n)}-\\eta\\frac{\\partial\\mathcal{R}}{\\partial w^k_{m_k}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The number of iterations of gradient descent is controlled by the _Epochs_ we chose a priori. A last comment about the learning method is about the kind of algorithm. So far we use the gradient descents where we basically sum over all the training dataset when computing the risk function. Other useful method are the __Stochastic Gradient Descents (SDG)__ where we run the backpropagation algorithm a number of _Epochs_ over a random subsample, called _Batch_, of the entire training set. There are also method of onlin e learnign where we update weights every observation. The use of SDG or online learning are useful to gain in computational cost and moreover to avoid to get in a local minimum. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a class=\"anchor\" id='s2'> Implementation in Python </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the MLP with the module `keras.models`:\n",
    "\n",
    "\n",
    "`keras.models.Sequential()`\n",
    "\n",
    "\n",
    "Since it's a class is composed by __parameters__, __attributes__ and __method__. The official page where all the details can be found is [here](https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a class=\"anchor\" id='s3'> Mini-Challenge </a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/heart.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>303</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "141   43    1   0       115   303    0        1      181      0      1.2   \n",
       "16    58    0   2       120   340    0        1      172      0      0.0   \n",
       "15    50    0   2       120   219    0        1      158      0      1.6   \n",
       "107   45    0   0       138   236    0        0      152      1      0.2   \n",
       "180   55    1   0       132   353    0        1      132      1      1.2   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "141      1   0     2       1  \n",
       "16       2   0     2       1  \n",
       "15       1   0     2       1  \n",
       "107      1   0     2       1  \n",
       "180      1   1     3       0  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop('target',axis=1), df.target, shuffle=True, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "n_cols = X_train.shape[1]\n",
    "\n",
    "model.add(Dense(10, activation='relu', input_dim=13))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.5826\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 52us/step - loss: 0.6660 - accuracy: 0.6033\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 51us/step - loss: 0.6445 - accuracy: 0.7066\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 53us/step - loss: 0.6229 - accuracy: 0.7603\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 50us/step - loss: 0.5974 - accuracy: 0.7934\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 54us/step - loss: 0.5724 - accuracy: 0.7851\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5443 - accuracy: 0.7851\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 57us/step - loss: 0.5170 - accuracy: 0.7769\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 59us/step - loss: 0.4896 - accuracy: 0.7893\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.4658 - accuracy: 0.7893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14a90bc50>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 0s 69us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4664972375650875, 0.7704917788505554]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14aecbf28>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, model.predict_classes(X_test)), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the SVC is 0.7704918032786885\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the SVC is',accuracy_score(y_test, model.predict_classes(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
